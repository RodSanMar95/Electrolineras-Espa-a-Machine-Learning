# -*- coding: utf-8 -*-
"""Machine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNmwp0kAuSWcyEEmwpyEwcX5zXyl_p64

# Prediction of Charging Infrastructure for Electric Vehicles
## Introduction

In this analysis, we aim to develop a predictive model that estimates the number of electric chargers needed based on the number of electric vehicles in a province. This type of prediction allows for efficient planning of charging infrastructure, ensuring that resources are well-distributed and available to meet future demand.

With the growth of electric mobility, it is essential for cities and regions to adapt their charging infrastructure to the number of electric vehicles in circulation. Our goal is to create a regression model that leverages current data on electric vehicles and chargers, as well as predictions made using other techniques, which allow us to forecast the number of electric vehicles in the future, to aid in strategic decision-making and facilitate the transition towards a more sustainable transportation system.

This project consists of several stages, including data loading and preprocessing, exploration of the relationship between variables, building and testing different models, and performance evaluation. Ultimately, the proposed model is expected to guide the planning of electric chargers accurately and data-driven.

## Data Processing

To develop this model, we have two key databases. The first, named “parque_vehiculos.db,” includes 45 different features describing the current vehicle fleet in Spain. The second database, “electrolineras.db,” contains detailed information about all the electric chargers operating in Spanish territory.

To achieve our goal, we will create a consolidated database that integrates information from both sources. This new database, named “BD_coches_cargadores.db,” will aggregate the number of vehicles and electric chargers by province. Once we have this database, we can proceed to build predictive models.

We import all the libraries we are going to use.
"""

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from category_encoders import BinaryEncoder
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
import xgboost as xgb
import joblib
from sklearn.model_selection import KFold, train_test_split

"""First, let’s analyze the database that contains the vehicle fleet."""

# We connect to the database.
db_path = 'parque_vehiculos.db'
conn = sqlite3.connect(db_path)

# We retrieve the table names.
cursor = conn.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()

# Dictionary to store the unique values per variable in each table.
unique_values_per_column = {}

# For each table, we retrieve the column names and the unique values.
for table_name in tables:
    table_name = table_name[0]  # We extract the table name.
    cursor.execute(f"PRAGMA table_info({table_name});")
    columns = cursor.fetchall()
    unique_values_per_column[table_name] = {}

    for column in columns:
        column_name = column[1]  # We extract the column name.
        cursor.execute(f"SELECT COUNT(DISTINCT {column_name}) FROM {table_name};")
        unique_count = cursor.fetchone()[0]
        unique_values_per_column[table_name][column_name] = unique_count


# We execute the query to obtain the distinct values from the column TIPO_DISTINTIVO.
cursor.execute(f"SELECT DISTINCT TIPO_DISTINTIVO FROM {table_name};")
distinct_values = cursor.fetchall()

# We close the connection.
conn.close()

# We convert the results to a DataFrame for better visualization
unique_values_df = pd.DataFrame(unique_values_per_column)

# We display the results
print(unique_values_df)


# We display the results
for value in distinct_values:
    print(value[0])

"""The database contains a wide variety of characteristics for each vehicle. However, we will focus particularly on the feature called “TIPO_DISTINTIVO”. This column classifies vehicles into five distinct categories: CERO, DISTINTIVO C, DISTINTIVO B, SIN DISTINTIVO, and ECO, which are assigned based on the emissions of each vehicle. Using this classification, it is possible to distinguish electric vehicles from non-electric ones.

To work exclusively with electric vehicle data, we will remove the rows where the value of the column “TIPO_DISTINTIVO” is not equal to “CERO”.
"""

# Connect to the database
db_path = 'parque_vehiculos.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Query to delete rows where TIPO_DISTINTIVO is not equal to 'CERO'
cursor.execute("DELETE FROM vehiculos_electricos WHERE TIPO_DISTINTIVO != 'CERO';")

# Commit the changes
conn.commit()

# Close the connection
conn.close()

print("All rows where TIPO_DISTINTIVO is not 'CERO' have been deleted.")

"""Once the non-electric vehicles are removed, we verify that this process has been carried out correctly."""

# Connect to the database
db_path = 'parque_vehiculos.db'
conn = sqlite3.connect(db_path)

# Get the table names
cursor = conn.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()

# Dictionary to store unique values per variable in each table
unique_values_per_column = {}

# For each table, get the column names and unique values
for table_name in tables:
    table_name = table_name[0]  # Extract the table name
    cursor.execute(f"PRAGMA table_info({table_name});")
    columns = cursor.fetchall()
    unique_values_per_column[table_name] = {}

    for column in columns:
        column_name = column[1]  # Extract the column name
        cursor.execute(f"SELECT COUNT(DISTINCT {column_name}) FROM {table_name};")
        unique_count = cursor.fetchone()[0]
        unique_values_per_column[table_name][column_name] = unique_count

# Execute the query to get distinct values of the column TIPO_DISTINTIVO
cursor.execute(f"SELECT DISTINCT TIPO_DISTINTIVO FROM {table_name};")
distinct_values = cursor.fetchall()

# Close the connection
conn.close()

# Convert the results to a DataFrame for better visualization
unique_values_df = pd.DataFrame(unique_values_per_column)

# Display the results
print(unique_values_df)

# Display the distinct values
for value in distinct_values:
    print(value[0])

"""As we can observe, the “TIPO_DISTINTIVO” column now contains only one unique value, which is “CERO”. Therefore, we can conclude that all vehicles stored in the database are electric vehicles.

After verifying the database data, we will create a new database to store the number of electric cars by province.
"""

# Reverse the dictionary to map province numbers to names
provinces_dict_reversed = {v: k for k, v in provinces_dict.items()}

# Connect to the original database (parque_vehiculos.db)
db_path = 'parque_vehiculos.db'
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Get the number of cars per province from the original database
cursor.execute("SELECT PROVINCIA, COUNT(*) as numero_coches FROM vehiculos_electricos GROUP BY PROVINCIA;")
cars_per_province = cursor.fetchall()

# Close the connection to the original database
conn.close()

# Prepare the data for insertion, ordered by province number
data_to_insert = []

for row in cars_per_province:
    province_number = row[0]
    car_count = row[1]

    # Get the province name using the reversed dictionary
    province_name = provinces_dict_reversed.get(province_number, 'Unknown')

    # Only add the data if the province number is found in the dictionary
    if province_name != 'Unknown':
        data_to_insert.append((province_number, province_name, car_count))
    else:
        print(f"Warning: Province number '{province_number}' not found in the reversed dictionary.")

# Sort the data by the province number
data_to_insert.sort(key=lambda x: x[0])

# Create and connect to the new database coches_por_provincia.db
new_db_path = 'coches_por_provincia.db'
new_conn = sqlite3.connect(new_db_path)
new_cursor = new_conn.cursor()

# Create the new table with province names
new_cursor.execute('''
    CREATE TABLE IF NOT EXISTS Coches_por_provincia_con_nombre (
        PROVINCIA_NUMERO INTEGER,
        PROVINCIA_NOMBRE TEXT,
        NUMERO_COCHES INTEGER
    );
''')

# Insert the sorted data into the new table
new_cursor.executemany('''
    INSERT INTO Coches_por_provincia_con_nombre (PROVINCIA_NUMERO, PROVINCIA_NOMBRE, NUMERO_COCHES)
    VALUES (?, ?, ?);
''', data_to_insert)

# Commit the changes
new_conn.commit()

# Close the connection to the new database
new_conn.close()

print("The database 'coches_por_provincia.db' has been successfully created with province names, ordered by province number.")

# Connect to the database coches_por_provincia.db
db_path = 'coches_por_provincia.db'
conn = sqlite3.connect(db_path)

# Read all data from the table Coches_por_provincia_con_nombre
query = "SELECT PROVINCIA_NOMBRE, NUMERO_COCHES FROM Coches_por_provincia_con_nombre"
df = pd.read_sql_query(query, conn)

# Close the connection
conn.close()

# Display the data in tabular format
print("Car data per province:")
print(df)

# Create the horizontal bar chart
plt.figure(figsize=(12, 8))
plt.barh(df['PROVINCIA_NOMBRE'], df['NUMERO_COCHES'], color='skyblue')
plt.ylabel('Province')
plt.xlabel('Number of Cars')
plt.title('Number of Cars per Province')
plt.tight_layout()  # Adjust to prevent text clipping
plt.show()

"""As we can observe, we now have our database with the number of electric cars by province. Additionally, we can see that the provinces of Madrid and Barcelona have significantly more cars than the other provinces, so we may need to treat them as outliers.

Next, we will proceed with the second database (“electrolineras.db”). To do so, we will first visualize the data within this database.
"""

# Connect to the database electrolineras.db
db_path = 'electrolineras.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Table name
table_name = 'electrolineras'

# Check if the table exists in the database
cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?;", (table_name,))
if cursor.fetchone():
    print(f"Table: {table_name}")

    # Get the columns of the table
    cursor.execute(f"PRAGMA table_info({table_name});")
    columns = cursor.fetchall()

    # For each column, display the number of data points (non-null rows) and the number of unique values
    for column in columns:
        column_name = column[1]  # Column name

        # Count the number of data points (non-null rows) in each column
        cursor.execute(f"SELECT COUNT({column_name}) FROM {table_name} WHERE {column_name} IS NOT NULL;")
        num_data = cursor.fetchone()[0]
        print(f"Column: {column_name} - Number of data points: {num_data}")

        # Count the number of unique values in each column
        cursor.execute(f"SELECT COUNT(DISTINCT {column_name}) FROM {table_name};")
        unique_count = cursor.fetchone()[0]
        print(f"Column: {column_name} - Unique values: {unique_count}")

    # If the 'provincia' column exists, display all its unique values
    cursor.execute(f"PRAGMA table_info({table_name});")
    column_names = [column[1] for column in cursor.fetchall()]
    if 'provincia' in column_names:
        cursor.execute(f"SELECT DISTINCT provincia FROM {table_name};")
        unique_provinces = cursor.fetchall()

        if unique_provinces:
            print("\nUnique values in the 'provincia' column:")
            for province in unique_provinces:
                print(province[0])
else:
    print(f"The table '{table_name}' does not exist in the database.")

# Close the connection
conn.close()

"""As we can observe, there are a total of 6,744 chargers distributed across different provinces.

Next, we will create a database named “electrolineras_por_provincia.db” where we will store the total number of electric chargers per province.
"""

# Dictionary that maps province names to their numbers, including alternative names
provinces_dict = {
    'Álava': 1, 'Araba/Álava': 1,
    'Albacete': 2,
    'Alicante': 3, 'Alicante/Alacant': 3,
    'Almería': 4,
    'Ávila': 5,
    'Badajoz': 6,
    'Islas Baleares': 7, 'Balears, Illes': 7,
    'Barcelona': 8,
    'Burgos': 9,
    'Cáceres': 10,
    'Cádiz': 11,
    'Castellón': 12, 'Castellón/Castelló': 12,
    'Ciudad Real': 13,
    'Córdoba': 14,
    'La Coruña': 15, 'Coruña, A': 15,
    'Cuenca': 16,
    'Gerona': 17, 'Girona': 17,
    'Granada': 18,
    'Guadalajara': 19,
    'Guipúzcoa': 20, 'Gipuzkoa': 20,
    'Huelva': 21,
    'Huesca': 22,
    'Jaén': 23,
    'León': 24,
    'Lérida': 25, 'Lleida': 25,
    'La Rioja': 26, 'Rioja, La': 26,
    'Lugo': 27,
    'Madrid': 28,
    'Málaga': 29,
    'Murcia': 30,
    'Navarra': 31,
    'Orense': 32, 'Ourense': 32,
    'Asturias': 33,
    'Palencia': 34,
    'Las Palmas': 35, 'Palmas, Las': 35,
    'Pontevedra': 36,
    'Salamanca': 37,
    'Santa Cruz de Tenerife': 38,
    'Cantabria': 39,
    'Segovia': 40,
    'Sevilla': 41,
    'Soria': 42,
    'Tarragona': 43,
    'Teruel': 44,
    'Toledo': 45,
    'Valencia': 46, 'Valencia/València': 46,
    'Valladolid': 47,
    'Vizcaya': 48, 'Bizkaia': 48,
    'Zamora': 49,
    'Zaragoza': 50,
    'Ceuta': 51,
    'Melilla': 52
}

# Connect to the database electrolineras.db
db_path = 'electrolineras.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Execute a query to count the number of charging stations per province
cursor.execute("SELECT provincia, COUNT(*) as numero_electrolineras FROM electrolineras GROUP BY provincia;")
charging_stations_per_province = cursor.fetchall()

# Close the connection to the original database
conn.close()

# Prepare the data for insertion, ordered by the province number
data_to_insert = []

for row in charging_stations_per_province:
    province_name = row[0]
    num_charging_stations = row[1]

    # Get the province number from the dictionary
    province_number = provinces_dict.get(province_name, None)

    # If the province number is found, add it to the list
    if province_number is not None:
        data_to_insert.append((province_name, province_number, num_charging_stations))
    else:
        print(f"Warning: Province number not found for {province_name}")

# Sort the data by the province number
data_to_insert.sort(key=lambda x: x[1])

# Create a new database called electrolineras_por_provincia.db
new_db_path = 'electrolineras_por_provincia.db'
new_conn = sqlite3.connect(new_db_path)

# Create a new table in the database electrolineras_por_provincia.db
new_cursor = new_conn.cursor()
new_cursor.execute('''
    CREATE TABLE IF NOT EXISTS Electrolineras_por_provincia (
        PROVINCIA_NOMBRE TEXT,
        PROVINCIA_NUMERO INTEGER,
        NUMERO_ELECTROLINERAS INTEGER
    );
''')

# Insert the sorted data into the new table
new_cursor.executemany('''
    INSERT INTO Electrolineras_por_provincia (PROVINCIA_NOMBRE, PROVINCIA_NUMERO, NUMERO_ELECTROLINERAS)
    VALUES (?, ?, ?);
''', data_to_insert)

# Commit the changes
new_conn.commit()

# Close the connection to the new database
new_conn.close()

print("The database 'electrolineras_por_provincia.db' has been successfully created with the number of charging stations per province, ordered by province number.")

"""Once this database is created, we will visualize it just as we did previously."""

# Connect to the database electrolineras_por_provincia.db
db_path = 'electrolineras_por_provincia.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Execute a query to retrieve all data from the table Electrolineras_por_provincia
cursor.execute("SELECT * FROM Electrolineras_por_provincia;")
data = cursor.fetchall()

# Get the column names
cursor.execute("PRAGMA table_info(Electrolineras_por_provincia);")
columns = [col[1] for col in cursor.fetchall()]

# Close the connection
conn.close()

# Convert the data to a DataFrame for better visualization
df = pd.DataFrame(data, columns=columns)

# Display the DataFrame
print(df)

# Create a bar chart with provinces on the Y-axis and the number of charging stations on the X-axis
plt.figure(figsize=(10, 8))
plt.barh(df['PROVINCIA_NOMBRE'], df['NUMERO_ELECTROLINERAS'], color='lightgreen')
plt.xlabel('Number of Charging Stations')
plt.ylabel('Province')
plt.title('Number of Charging Stations per Province')

# Show the chart
plt.tight_layout()
plt.show()

"""As we can observe, all the data is correctly loaded, and similar to the number of cars by province, Madrid and Barcelona are the provinces with the highest number of chargers and may potentially be considered as outliers.

Next, we will merge the two created databases to have both the number of chargers and vehicles by province in a single database, which will allow us to build predictive models.
"""

# Connect to the databases electrolineras_por_provincia.db and coches_por_provincia.db
db_electrolineras_path = 'electrolineras_por_provincia.db'
db_coches_path = 'coches_por_provincia.db'

# Connect to the electrolineras database
conn_electrolineras = sqlite3.connect(db_electrolineras_path)
cursor_electrolineras = conn_electrolineras.cursor()

# Retrieve data of charging stations per province
cursor_electrolineras.execute("SELECT PROVINCIA_NUMERO, PROVINCIA_NOMBRE, NUMERO_ELECTROLINERAS FROM Electrolineras_por_provincia;")
charging_stations_per_province = cursor_electrolineras.fetchall()

# Close the connection to the electrolineras database
conn_electrolineras.close()

# Connect to the coches database
conn_coches = sqlite3.connect(db_coches_path)
cursor_coches = conn_coches.cursor()

# Retrieve data of cars per province
cursor_coches.execute("SELECT PROVINCIA_NUMERO, PROVINCIA_NOMBRE, NUMERO_COCHES FROM Coches_por_provincia_con_nombre;")
cars_per_province = cursor_coches.fetchall()

# Close the connection to the coches database
conn_coches.close()

# Create a dictionary to store data of cars and charging stations per province
province_data = {}

# Insert data of charging stations into the dictionary
for row in charging_stations_per_province:
    province_number = row[0]  # Province number
    province_name = row[1]  # Province name
    num_charging_stations = row[2]  # Number of charging stations
    province_data[province_number] = {
        'province_name': province_name,
        'num_charging_stations': num_charging_stations,
        'num_cars': 0  # Initialize with 0, will be updated with car data
    }

# Insert car data into the dictionary, ensuring to merge the information
for row in cars_per_province:
    province_number = row[0]  # Province number
    province_name = row[1]  # Province name
    num_cars = row[2]  # Number of cars
    if province_number in province_data:
        province_data[province_number]['num_cars'] = num_cars
    else:
        # If the province was not in charging stations data, add it
        province_data[province_number] = {
            'province_name': province_name,
            'num_charging_stations': 0,  # No charging station data for this province
            'num_cars': num_cars
        }

# Create the new database BD_Coches_Cargadores.db
new_db_path = 'BD_Coches_Cargadores.db'
new_conn = sqlite3.connect(new_db_path)

# Create a new table in the BD_Coches_Cargadores.db database
new_cursor = new_conn.cursor()
new_cursor.execute('''
    CREATE TABLE IF NOT EXISTS Coches_Cargadores_por_provincia (
        PROVINCIA_NUMERO INTEGER,
        PROVINCIA_NOMBRE TEXT,
        NUMERO_ELECTROLINERAS INTEGER,
        NUMERO_COCHES INTEGER
    );
''')

# Insert the data into the new table
for province_number, data in province_data.items():
    new_cursor.execute('''
        INSERT INTO Coches_Cargadores_por_provincia (PROVINCIA_NUMERO, PROVINCIA_NOMBRE, NUMERO_ELECTROLINERAS, NUMERO_COCHES)
        VALUES (?, ?, ?, ?);''',
        (province_number, data['province_name'], data['num_charging_stations'], data['num_cars']))

# Commit the changes
new_conn.commit()

# Close the connection to the new database
new_conn.close()

print("The database 'BD_Coches_Cargadores.db' has been successfully created with combined data of cars and charging stations per province.")

"""Once the database is created, we review the data it contains."""

# Connect to the database BD_Coches_Cargadores.db
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Execute a query to retrieve all data from the table Coches_Cargadores_por_provincia
cursor.execute("SELECT * FROM Coches_Cargadores_por_provincia;")
data = cursor.fetchall()

# Get the column names
cursor.execute("PRAGMA table_info(Coches_Cargadores_por_provincia);")
columns = [col[1] for col in cursor.fetchall()]

# Close the connection
conn.close()

# Convert the data to a DataFrame for better visualization
df = pd.DataFrame(data, columns=columns)

# Display the DataFrame
print(df)

# Create a scatter plot with cars on the X-axis and charging stations on the Y-axis
plt.figure(figsize=(12, 6))

# Use a colormap to assign a different color to each province
provinces = df['PROVINCIA_NOMBRE'].unique()
colormap = plt.colormaps['tab20']
colors = colormap(np.linspace(0, 1, len(provinces)))

for i, province in enumerate(provinces):
    province_data = df[df['PROVINCIA_NOMBRE'] == province]
    plt.scatter(province_data['NUMERO_COCHES'], province_data['NUMERO_ELECTROLINERAS'],
                color=colors[i], label=province, s=100)

# Axis labels and title
plt.xlabel('Number of Cars')
plt.ylabel('Number of Charging Stations')
plt.title('Relationship between Cars and Charging Stations per Province')

# Display the legend
plt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Province')

# Adjust the space around the plot
plt.subplots_adjust(left=0.1, right=0.7, top=0.9, bottom=0.1)

# Show the plot
plt.show()

"""Reviewing the data, we can see that the provinces of Ceuta and Melilla do not contain information about the number of charging stations. Therefore, we will need to remove these rows. Additionally, we will eliminate the data for the provinces of Madrid and Barcelona, as we have considered them outliers."""

# Connect to the database BD_Coches_Cargadores.db
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Execute a query to delete rows where NUMERO_ELECTROLINERAS is 0 and the province is Ceuta or Melilla
# Additionally, remove rows for Madrid and Barcelona
cursor.execute("""
    DELETE FROM Coches_Cargadores_por_provincia
    WHERE (PROVINCIA_NOMBRE = 'Ceuta' OR PROVINCIA_NOMBRE = 'Melilla') AND NUMERO_ELECTROLINERAS = 0
    OR PROVINCIA_NOMBRE = 'Madrid'
    OR PROVINCIA_NOMBRE = 'Barcelona';
""")

# Commit the changes
conn.commit()

# Close the connection
conn.close()

print("Rows for Ceuta and Melilla without charging station data, as well as rows for Madrid and Barcelona, have been deleted.")

# Connect to the database BD_Coches_Cargadores.db
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Create a cursor
cursor = conn.cursor()

# Execute a query to retrieve all data from the table Coches_Cargadores_por_provincia
cursor.execute("SELECT * FROM Coches_Cargadores_por_provincia;")
data = cursor.fetchall()

# Get the column names
cursor.execute("PRAGMA table_info(Coches_Cargadores_por_provincia);")
columns = [col[1] for col in cursor.fetchall()]

# Close the connection
conn.close()

# Convert the data to a DataFrame for better visualization
df = pd.DataFrame(data, columns=columns)

# Display the DataFrame
print(df)

# Create a scatter plot with cars on the X-axis and charging stations on the Y-axis
plt.figure(figsize=(12, 6))

# Use a colormap to assign a different color to each province
provinces = df['PROVINCIA_NOMBRE'].unique()
colormap = plt.colormaps['tab20']
colors = colormap(np.linspace(0, 1, len(provinces)))

for i, province in enumerate(provinces):
    province_data = df[df['PROVINCIA_NOMBRE'] == province]
    plt.scatter(province_data['NUMERO_COCHES'], province_data['NUMERO_ELECTROLINERAS'],
                color=colors[i], label=province, s=100)

# Axis labels and title
plt.xlabel('Number of Cars')
plt.ylabel('Number of Charging Stations')
plt.title('Relationship between Cars and Charging Stations per Province')

# Display the legend
plt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Province')

# Adjust the space around the plot
plt.subplots_adjust(left=0.1, right=0.7, top=0.9, bottom=0.1)

# Show the plot
plt.show()

"""We can now see a clear relationship in the data, and we have successfully removed the outliers.

Next, we will use binary encoding for the province column. We chose this method because it prevents certain provinces from having more weight than others and avoids creating a large number of new columns.
"""

# Connect to the database BD_Coches_Cargadores.db
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load data from the table
query = "SELECT PROVINCIA_NOMBRE, NUMERO_COCHES, NUMERO_ELECTROLINERAS FROM Coches_Cargadores_por_provincia"
df = pd.read_sql(query, conn)

# Temporarily close the connection
conn.close()

# Perform Binary Encoding on the 'PROVINCIA_NOMBRE' column
encoder = BinaryEncoder(cols=['PROVINCIA_NOMBRE'])
df_encoded = encoder.fit_transform(df)

# Save the updated database
conn = sqlite3.connect(db_path)

# Overwrite the existing table with the new encoded data
df_encoded.to_sql('Coches_Cargadores_por_provincia_binary_encoded', conn, if_exists='replace', index=False)

# Close the connection
conn.close()

# Display all data from the modified table
conn = sqlite3.connect(db_path)
df_updated = pd.read_sql("SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded", conn)

# Close the connection
conn.close()

# Display all data
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)  # Show all columns
print(df_updated)

"""As we can see, we have created the binary encoding. A total of 5 columns have been generated, allowing us to use the province feature in our models.

We can now proceed to build our first machine learning model. We will start with a linear regression model, initially we will split the data in train and test to evaluate the functioning of our model, additionally, we will apply cross validation with K-Fold due to the lack of excesive data. Additionally, we will normalize the data to ensure they are on the same scale, which should improve model performance. Finally, we will apply L1 and L2 regularizers to prevent overfitting and use loops to select the best values for the hyperparameters.
"""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load data from the table
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate independent variables (X) and the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])
y = df['NUMERO_ELECTROLINERAS']

# Initial split into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
normalizer = MinMaxScaler()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# Define K-Fold Cross Validation for the training set
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Store results
ridge_mse_scores = []
ridge_r2_scores = []
lasso_mse_scores = []
lasso_r2_scores = []

# Optimize and evaluate models with Cross Validation
best_alpha_ridge = None
best_alpha_lasso = None
best_mse_ridge = float('inf')
best_mse_lasso = float('inf')

# Alpha values to test
alphas = [0.005, 0.01, 0.05, 0.1, 1.0, 10.0, 100.0, 1000.0]

# Evaluate Ridge with Cross Validation on the training set
for alpha in alphas:
    mse_scores_ridge = []
    r2_scores_ridge = []

    for train_index, val_index in kf.split(X_train_normalized):
        X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        model_ridge = Ridge(alpha=alpha)
        model_ridge.fit(X_train_fold, y_train_fold)

        y_val_pred = model_ridge.predict(X_val_fold)
        mse_scores_ridge.append(mean_squared_error(y_val_fold, y_val_pred))
        r2_scores_ridge.append(r2_score(y_val_fold, y_val_pred))

    mean_mse_ridge = np.mean(mse_scores_ridge)
    mean_r2_ridge = np.mean(r2_scores_ridge)

    if mean_mse_ridge < best_mse_ridge:
        best_mse_ridge = mean_mse_ridge
        best_alpha_ridge = alpha

    ridge_mse_scores.append(mean_mse_ridge)
    ridge_r2_scores.append(mean_r2_ridge)

print(f"Ridge Regression:")
print(f"Best alpha: {best_alpha_ridge}")
print(f"Mean MSE (Ridge): {best_mse_ridge}")
print(f"Mean R² (Ridge): {np.mean(ridge_r2_scores)}")
print("-" * 50)

# Evaluate Lasso with Cross Validation on the training set
for alpha in alphas:
    mse_scores_lasso = []
    r2_scores_lasso = []

    for train_index, val_index in kf.split(X_train_normalized):
        X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        model_lasso = Lasso(alpha=alpha, max_iter=10000)
        model_lasso.fit(X_train_fold, y_train_fold)

        y_val_pred = model_lasso.predict(X_val_fold)
        mse_scores_lasso.append(mean_squared_error(y_val_fold, y_val_pred))
        r2_scores_lasso.append(r2_score(y_val_fold, y_val_pred))

    mean_mse_lasso = np.mean(mse_scores_lasso)
    mean_r2_lasso = np.mean(r2_scores_lasso)

    if mean_mse_lasso < best_mse_lasso:
        best_mse_lasso = mean_mse_lasso
        best_alpha_lasso = alpha

    lasso_mse_scores.append(mean_mse_lasso)
    lasso_r2_scores.append(mean_r2_lasso)

print(f"Lasso Regression:")
print(f"Best alpha: {best_alpha_lasso}")
print(f"Mean MSE (Lasso): {best_mse_lasso}")
print(f"Mean R² (Lasso): {np.mean(lasso_r2_scores)}")
print("-" * 50)

# Evaluate the performance on the final test set with the best Ridge and Lasso models
best_model_ridge = Ridge(alpha=best_alpha_ridge)
best_model_ridge.fit(X_train_normalized, y_train)
y_test_pred_ridge = best_model_ridge.predict(X_test_normalized)

test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)
test_r2_ridge = r2_score(y_test, y_test_pred_ridge)

best_model_lasso = Lasso(alpha=best_alpha_lasso, max_iter=10000)
best_model_lasso.fit(X_train_normalized, y_train)
y_test_pred_lasso = best_model_lasso.predict(X_test_normalized)

test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)
test_r2_lasso = r2_score(y_test, y_test_pred_lasso)

print(f"Test MSE (Ridge): {test_mse_ridge}")
print(f"Test R² (Ridge): {test_r2_ridge}")
print("-" * 50)
print(f"Test MSE (Lasso): {test_mse_lasso}")
print(f"Test R² (Lasso): {test_r2_lasso}")
print("-" * 50)

# Save the best models and the normalizer
model_ridge_path = 'Regression_Model.pkl'
model_lasso_path = 'Lasso_Regression_Model.pkl'
scaler_path = 'Normalizer.pkl'

joblib.dump(best_model_ridge, model_ridge_path)
joblib.dump(best_model_lasso, model_lasso_path)
joblib.dump(normalizer, scaler_path)

print(f"Ridge model saved successfully as '{model_ridge_path}'.")
print(f"Lasso model saved successfully as '{model_lasso_path}'.")
print(f"Normalizer saved successfully as '{scaler_path}'.")

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(alphas, ridge_mse_scores, marker='o', label='Ridge MSE (Cross-Validation)')
plt.plot(alphas, lasso_mse_scores, marker='x', label='Lasso MSE (Cross-Validation)')
plt.xscale('log')
plt.xlabel('Alpha (log scale)')
plt.ylabel('Mean Squared Error')
plt.title('MSE vs Alpha for Ridge and Lasso Regression')
plt.legend()
plt.show()

"""After evaluating both Ridge and Lasso regression models using K-Fold cross-validation, the Ridge model exhibited superior performance. The optimal regularization parameter (alpha) for Ridge resulted in the lowest Mean Squared Error (MSE) and the highest R² score on the test dataset. This suggests that Ridge regression effectively managed multicollinearity and overfitting, providing a better fit for the data compared to Lasso regression.

Next, we will try using a random forest model and compare the results obtained. As with the linear regression model, we will divide the data into 80% train and 20% test sets. Additionally, we will normalize the data to ensure they are on the same scale and apply cross validation.
"""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load data from the table with Binary Encoding and normalized data
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate the independent variables (X) from the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable

# Split data: 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
normalizer = MinMaxScaler()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Initialize lists to store cross-validation results
mse_scores = []
r2_scores = []

# Perform K-Fold Cross Validation
for train_index, val_index in kf.split(X_train_normalized):
    X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    # Create and train the Random Forest model
    rf_model = RandomForestRegressor(n_estimators=500, max_depth=20, random_state=42)
    rf_model.fit(X_train_fold, y_train_fold)

    # Predictions on the validation set
    y_val_pred = rf_model.predict(X_val_fold)

    # Calculate Mean Squared Error and R² score for the validation set
    mse = mean_squared_error(y_val_fold, y_val_pred)
    r2 = r2_score(y_val_fold, y_val_pred)

    # Store the results
    mse_scores.append(mse)
    r2_scores.append(r2)

# Calculate the average metrics from cross-validation
mean_mse_cv = np.mean(mse_scores)
mean_r2_cv = np.mean(r2_scores)

print(f"Cross-Validation Results (K-Fold):")
print(f"Average MSE: {mean_mse_cv}")
print(f"Average R²: {mean_r2_cv}")
print("-" * 50)

# Train the final Random Forest model on the entire training set
final_rf_model = RandomForestRegressor(n_estimators=500, max_depth=20, random_state=42)
final_rf_model.fit(X_train_normalized, y_train)

# Predictions on the training set
y_train_pred = final_rf_model.predict(X_train_normalized)

# Predictions on the test set
y_test_pred = final_rf_model.predict(X_test_normalized)

# Calculate errors (Mean Squared Error and R²) for both training and test sets
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print the results
print(f"Mean Squared Error on the training set: {train_mse}")
print(f"Mean Squared Error on the test set: {test_mse}")
print(f"R² on the training set: {train_r2}")
print(f"R² on the test set: {test_r2}")

# Create scatter plots to visualize the results

# Plot for the training set
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Training')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')
plt.title('Predictions vs Actual Values (Training Set)')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

# Plot for the test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', label='Test')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')
plt.title('Predictions vs Actual Values (Test Set)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

"""Next, we will proceed to optimize the hyperparameters to obtain the most accurate Random Forest model. We will implement a series of loops to train the model with different combinations of hyperparameters, selecting the best values based on their performance. The hyperparameters to tune are as follows:

	•	n_estimators: Number of trees in the forest. We will test values of 25, 50, 100, 300, and 500.
	•	max_depth: Maximum depth of each tree. We will evaluate values of 3, 5, 10, 20, and 30.
	•	min_samples_split: Minimum number of samples required to split a node. We will explore values of 2, 5, and 10.
	•	min_samples_leaf: Minimum number of samples at the leaf nodes, with values of 1, 2, and 4.
	•	max_features: Maximum number of features considered at each split. We will analyze options of ‘sqrt’ and ‘log2’.

This systematic approach will allow us to identify the combination of hyperparameters that maximizes the model’s performance, ensuring an optimal balance between accuracy and efficiency.
"""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load data from the table with Binary Encoding and normalized data
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate the independent variables (X) from the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable

# Split data: 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
normalizer = MinMaxScaler()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Hyperparameter ranges to tune
n_estimators_range = [25, 50, 100, 300, 500]
max_depth_range = [3, 5, 10, 20, 30]
min_samples_split_range = [2, 5, 10]
min_samples_leaf_range = [1, 2, 4]
max_features_range = ['sqrt', 'log2']

# Initialize variables for the best model
best_mse = float("inf")
best_params = {}

# Grid search over the hyperparameters using K-Fold Cross Validation
for n_estimators in n_estimators_range:
    for max_depth in max_depth_range:
        for min_samples_split in min_samples_split_range:
            for min_samples_leaf in min_samples_leaf_range:
                for max_features in max_features_range:
                    # Initialize list to store MSE scores for each fold
                    fold_mse_scores = []

                    # Perform K-Fold Cross Validation
                    for train_index, val_index in kf.split(X_train_normalized):
                        X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

                        # Configure the model with the current hyperparameters
                        rf_model = RandomForestRegressor(
                            n_estimators=n_estimators,
                            max_depth=max_depth,
                            min_samples_split=min_samples_split,
                            min_samples_leaf=min_samples_leaf,
                            max_features=max_features,
                            random_state=42
                        )

                        # Train the model on the fold
                        rf_model.fit(X_train_fold, y_train_fold)

                        # Prediction and MSE calculation on the validation fold
                        y_val_pred = rf_model.predict(X_val_fold)
                        fold_mse = mean_squared_error(y_val_fold, y_val_pred)
                        fold_mse_scores.append(fold_mse)

                    # Calculate the average MSE across all folds
                    mean_mse = np.mean(fold_mse_scores)

                    # Save the best hyperparameters if the average MSE is lower
                    if mean_mse < best_mse:
                        best_mse = mean_mse
                        best_params = {
                            'n_estimators': n_estimators,
                            'max_depth': max_depth,
                            'min_samples_split': min_samples_split,
                            'min_samples_leaf': min_samples_leaf,
                            'max_features': max_features
                        }

# Results
print("Best hyperparameters found:")
print(best_params)
print(f"Best average MSE from Cross-Validation: {best_mse}")

# Train the final model with the best hyperparameters on the entire training set
final_rf_model = RandomForestRegressor(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    min_samples_leaf=best_params['min_samples_leaf'],
    max_features=best_params['max_features'],
    random_state=42
)
final_rf_model.fit(X_train_normalized, y_train)

# Evaluate the final model on the test set
y_test_pred = final_rf_model.predict(X_test_normalized)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f"Test MSE: {test_mse}")

"""Once the optimal values for these hyperparameters have been identified, we will proceed to train the model with them."""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load the data from the table with Binary Encoding
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Separate independent variables (X) and the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable

# Split the data into 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data using MinMaxScaler
normalizer = MinMaxScaler()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Initialize lists to store cross-validation results
cv_mse_scores = []
cv_r2_scores = []

# Perform K-Fold Cross Validation
for train_index, val_index in kf.split(X_train_normalized):
    X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    # Create and train the Random Forest model with the chosen hyperparameters
    rf_model = RandomForestRegressor(
        n_estimators=25,
        max_depth=5,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42
    )
    rf_model.fit(X_train_fold, y_train_fold)

    # Predictions on the validation fold
    y_val_pred = rf_model.predict(X_val_fold)

    # Calculate Mean Squared Error and R² for the validation fold
    val_mse = mean_squared_error(y_val_fold, y_val_pred)
    val_r2 = r2_score(y_val_fold, y_val_pred)

    # Store the results
    cv_mse_scores.append(val_mse)
    cv_r2_scores.append(val_r2)

# Calculate the average metrics from cross-validation
mean_cv_mse = np.mean(cv_mse_scores)
mean_cv_r2 = np.mean(cv_r2_scores)

print(f"Cross-Validation Results (K-Fold):")
print(f"Average MSE: {mean_cv_mse}")
print(f"Average R²: {mean_cv_r2}")
print("-" * 50)

# Train the final model on the entire training set
final_rf_model = RandomForestRegressor(
    n_estimators=50,
    max_depth=10,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=42
)
final_rf_model.fit(X_train_normalized, y_train)

# Predictions on the training set
y_train_pred = final_rf_model.predict(X_train_normalized)

# Predictions on the test set
y_test_pred = final_rf_model.predict(X_test_normalized)

# Calculate the errors (Mean Squared Error and R²) for both training and test sets
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print the results
print(f"Mean Squared Error on training set: {train_mse}")
print(f"Mean Squared Error on test set: {test_mse}")
print(f"R² on training set: {train_r2}")
print(f"R² on test set: {test_r2}")

# Scatter plot for the training set
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Training')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Training Set)')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

# Scatter plot for the test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', label='Test')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Test Set)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

"""As we can see, the hyperparameter tuning did not work correctly, and we can conclude that this method is not suitable for analyzing this problem.

Therefore, we will proceed to try another type of model. In this case, we will use an XGBoost regressor model, which we will tune for optimal performance. Firstly, we will create a loop to identify the optimal hyperparameters for this model.
"""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load the data from the table with Binary Encoding
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate independent variables (X) and the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable

# Split the data into 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data using MinMaxScaler
normalizer = MinMaxScaler()
X_train_normalized = normalizer.fit_transform(X_train)
X_test_normalized = normalizer.transform(X_test)

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Define the hyperparameter ranges
n_estimators_options = [100, 300, 500, 700, 750, 800, 850, 900]
max_depth_options = [2, 3, 4, 5]
learning_rate_options = [0.01, 0.05, 0.1, 0.2]
subsample_options = [0.6, 0.8, 1.0]
colsample_bytree_options = [0.8, 1.0]
min_child_weight_options = [1, 3, 5]

# Variables to store the best results
best_params = None
best_mse = float("inf")

# Grid search loop with cross-validation
for n_estimators in n_estimators_options:
    for max_depth in max_depth_options:
        for learning_rate in learning_rate_options:
            for subsample in subsample_options:
                for colsample_bytree in colsample_bytree_options:
                    for min_child_weight in min_child_weight_options:
                        # Initialize list to store MSE for each fold
                        fold_mse_scores = []

                        # Perform K-Fold Cross Validation
                        for train_index, val_index in kf.split(X_train_normalized):
                            X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
                            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

                            # Configure the XGBoost model with the current hyperparameters
                            xgboost_model = XGBRegressor(
                                objective='reg:squarederror',
                                n_estimators=n_estimators,
                                max_depth=max_depth,
                                learning_rate=learning_rate,
                                subsample=subsample,
                                colsample_bytree=colsample_bytree,
                                min_child_weight=min_child_weight,
                                random_state=42
                            )

                            # Train the model on the fold
                            xgboost_model.fit(X_train_fold, y_train_fold)

                            # Predictions on the validation fold
                            y_val_pred = xgboost_model.predict(X_val_fold)

                            # Calculate Mean Squared Error for the validation fold
                            fold_mse = mean_squared_error(y_val_fold, y_val_pred)
                            fold_mse_scores.append(fold_mse)

                        # Calculate the average MSE across all folds
                        mean_mse = np.mean(fold_mse_scores)

                        # Update the best hyperparameters if a lower average MSE is found
                        if mean_mse < best_mse:
                            best_mse = mean_mse
                            best_params = {
                                'n_estimators': n_estimators,
                                'max_depth': max_depth,
                                'learning_rate': learning_rate,
                                'subsample': subsample,
                                'colsample_bytree': colsample_bytree,
                                'min_child_weight': min_child_weight
                            }

# Print the best hyperparameters and the associated MSE
print("Best hyperparameters found:")
print(best_params)
print(f"Best average MSE from Cross-Validation: {best_mse}")

# Train the final model with the best hyperparameters on the entire training set
final_xgboost_model = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    learning_rate=best_params['learning_rate'],
    subsample=best_params['subsample'],
    colsample_bytree=best_params['colsample_bytree'],
    min_child_weight=best_params['min_child_weight'],
    random_state=42
)
final_xgboost_model.fit(X_train_normalized, y_train)

# Predictions on the test set
y_test_pred = final_xgboost_model.predict(X_test_normalized)

# Calculate Mean Squared Error on the test set
test_mse = mean_squared_error(y_test, y_test_pred)

# Print the final test set performance
print(f"Test MSE: {test_mse}")

"""Once we know the optimum hyperparameters for our XGBOOST model, we can train our model and test how good it will be."""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load the data from the table with Binary Encoding
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate independent variables (X) and the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable (number of chargers)

# Split the data into 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Define the XGBoost model with the specified hyperparameters
xgboost_model = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=700,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.6,
    colsample_bytree=1.0,
    min_child_weight=1,
    random_state=42
)

# Initialize lists to store cross-validation results
cv_mse_scores = []
cv_r2_scores = []

# Perform K-Fold Cross Validation
for train_index, val_index in kf.split(X_train_normalized):
    X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    # Train the model on the current fold
    xgboost_model.fit(X_train_fold, y_train_fold)

    # Make predictions on the validation fold
    y_val_pred = xgboost_model.predict(X_val_fold)

    # Calculate Mean Squared Error and R² score for the validation fold
    val_mse = mean_squared_error(y_val_fold, y_val_pred)
    val_r2 = r2_score(y_val_fold, y_val_pred)

    # Store the results
    cv_mse_scores.append(val_mse)
    cv_r2_scores.append(val_r2)

# Calculate the average metrics from cross-validation
mean_cv_mse = np.mean(cv_mse_scores)
mean_cv_r2 = np.mean(cv_r2_scores)

# Print the cross-validation results
print("Cross-Validation Results (K-Fold):")
print(f"Average MSE: {mean_cv_mse}")
print(f"Average R²: {mean_cv_r2}")
print("-" * 50)

# Train the final model on the entire training set
xgboost_model.fit(X_train_normalized, y_train)

# Make predictions on the training set
y_train_pred = xgboost_model.predict(X_train_normalized)

# Make predictions on the test set
y_test_pred = xgboost_model.predict(X_test_normalized)

# Calculate the errors (Mean Squared Error and R²) for both training and test sets
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print the results
print(f"Mean Squared Error on training set: {train_mse}")
print(f"Mean Squared Error on test set: {test_mse}")
print(f"R² on training set: {train_r2}")
print(f"R² on test set: {test_r2}")

# Scatter plot for the training set
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Training')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Training Set)')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

# Scatter plot for the test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', label='Test')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Test Set)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

"""While this model demonstrates strong performance, further hyperparameter optimization is necessary to achieve the best possible results."""

# Connect to the database
db_path = 'BD_Coches_Cargadores.db'
conn = sqlite3.connect(db_path)

# Load the data from the table with Binary Encoding
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate independent variables (X) and the dependent variable (y)
X = df.drop(columns=['NUMERO_ELECTROLINERAS'])  # Independent variables
y = df['NUMERO_ELECTROLINERAS']  # Dependent variable (number of chargers)

# Split the data into 80% train and 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler()
X_train_normalized = scaler.fit_transform(X_train)
X_test_normalized = scaler.transform(X_test)

# Define the ranges of hyperparameters for L1 (reg_alpha) and L2 (reg_lambda) regularization
reg_alpha_options = [0, 0.1, 1, 10]
reg_lambda_options = [0, 0.05, 0.1, 1, 10]

# Define K-Fold Cross Validation
kf = KFold(n_splits=3, shuffle=True, random_state=42)

# Variables to store the best results
best_params = None
best_mse = float("inf")

# Hyperparameter search for L1 (reg_alpha) and L2 (reg_lambda)
for reg_alpha in reg_alpha_options:
    for reg_lambda in reg_lambda_options:
        # Initialize list to store cross-validation MSE scores
        cv_mse_scores = []

        # Perform K-Fold Cross Validation
        for train_index, val_index in kf.split(X_train_normalized):
            X_train_fold, X_val_fold = X_train_normalized[train_index], X_train_normalized[val_index]
            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

            # Define the XGBoost model with the current hyperparameters
            xgboost_model = xgb.XGBRegressor(
                objective='reg:squarederror',
                n_estimators=700,
                max_depth=5,
                learning_rate=0.05,
                subsample=0.6,
                colsample_bytree=1.0,
                min_child_weight=1,
                reg_alpha=reg_alpha,
                reg_lambda=reg_lambda,
                random_state=42
            )

            # Train the model on the current fold
            xgboost_model.fit(X_train_fold, y_train_fold)

            # Make predictions on the validation fold
            y_val_pred = xgboost_model.predict(X_val_fold)

            # Calculate Mean Squared Error for the validation fold
            fold_mse = mean_squared_error(y_val_fold, y_val_pred)
            cv_mse_scores.append(fold_mse)

        # Calculate the average MSE across all folds
        mean_cv_mse = np.mean(cv_mse_scores)

        # Update the best hyperparameters if a lower MSE is found
        if mean_cv_mse < best_mse:
            best_mse = mean_cv_mse
            best_params = {
                'reg_alpha': reg_alpha,
                'reg_lambda': reg_lambda
            }

# Print the best hyperparameters and the associated MSE
print(f"Best hyperparameters: {best_params}")
print(f"Best average MSE from Cross-Validation: {best_mse}")

# Train the final model with the best hyperparameters on the entire training set
best_xgboost_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=700,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.6,
    colsample_bytree=1.0,
    min_child_weight=1,
    reg_alpha=best_params['reg_alpha'],
    reg_lambda=best_params['reg_lambda'],
    random_state=42
)

best_xgboost_model.fit(X_train_normalized, y_train)

# Save the model using joblib
joblib.dump(best_xgboost_model, 'Predictor_Cargadores.pkl')
print("Model successfully saved as 'Predictor_Cargadores.pkl'.")

# Make predictions using the best model
y_train_pred = best_xgboost_model.predict(X_train_normalized)
y_test_pred = best_xgboost_model.predict(X_test_normalized)

# Calculate errors (MSE and R²)
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

# Print the final model results
print(f"Final Mean Squared Error on training set: {train_mse}")
print(f"Final Mean Squared Error on test set: {test_mse}")
print(f"Final R² on training set: {train_r2}")
print(f"Final R² on test set: {test_r2}")

# Scatter plot for the training set
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Training')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Training Set)')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

# Scatter plot for the test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', label='Test')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('Predictions vs True Values (Test Set)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.legend()
plt.grid(True)
plt.show()

"""The results indicate that the optimal choice for minimizing losses is to use an L2 regularizer. There are still signs of potential overfitting and there arent the lowest test errors we have observed so far. Therefore, we will select the liniar regression model as the most optimum one for future use.

Once the final model is established, a prediction phase will follow, requiring an estimation of the growth in the number of electric vehicles in the upcoming years. To achieve this, we will leverage the forecasts generated by our other model. According to these projections, the number of electric vehicles is expected to increase by approximately 9.1% over the next year. Consequently, we will base our predictions on an anticipated growth rate of 9.1% across all provinces.
"""

# Connect to the database BD_Prediccion.db
db_path = 'BD_Prediccion.db'
conn = sqlite3.connect(db_path)

# Load data from the table
query = "SELECT PROVINCIA_NOMBRE, NUMERO_COCHES, NUMERO_ELECTROLINERAS FROM Coches_Cargadores_por_provincia"
df = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Display all data from the modified table
conn = sqlite3.connect(db_path)
df_updated = pd.read_sql("SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded", conn)

# Close the connection
conn.close()

# Display all data
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)  # Show all columns
print(df_updated)

import sqlite3
import joblib
import pandas as pd

# Load the saved Ridge regression model and normalizer
model_path = 'Regression_Model.pkl'
scaler_path = 'Normalizer.pkl'

ridge_model = joblib.load(model_path)
normalizer = joblib.load(scaler_path)

print("Model and normalizer loaded successfully.")

# Connect to the new database where the updated data is stored
db_path = 'BD_Prediccion.db'
conn = sqlite3.connect(db_path)

# Load the new data (with augmented car numbers) from the table
query = "SELECT * FROM Coches_Cargadores_por_provincia_binary_encoded"
df = pd.read_sql(query, conn)
conn.close()

# Separate the independent variables (X) and keep a copy of the original DataFrame
X_new = df.drop(columns=['NUMERO_ELECTROLINERAS'])

# Normalize the new data using the previously saved normalizer
X_new_normalized = normalizer.transform(X_new)

# Make predictions using the loaded Ridge model
y_predicted = ridge_model.predict(X_new_normalized)

# Add the predicted number of charging stations as a new column
df['Prediccion_Cargadores'] = y_predicted

# Save the updated DataFrame back into the database
conn = sqlite3.connect(db_path)
df.to_sql('Coches_Cargadores_Prediccion', conn, if_exists='replace', index=False)
conn.close()

print("Predictions saved successfully to the 'Coches_Cargadores_Prediccion' table in the 'BD_Coches_Cargadores.db' database.")

# Dictionary mapping province numbers to names
provinces_dict = {
    1: 'Álava', 2: 'Albacete', 3: 'Alicante', 4: 'Almería', 5: 'Ávila',
    6: 'Badajoz', 7: 'Islas Baleares', 8: 'Barcelona', 9: 'Burgos', 10: 'Cáceres',
    11: 'Cádiz', 12: 'Castellón', 13: 'Ciudad Real', 14: 'Córdoba', 15: 'La Coruña',
    16: 'Cuenca', 17: 'Gerona', 18: 'Granada', 19: 'Guadalajara', 20: 'Guipúzcoa',
    21: 'Huelva', 22: 'Huesca', 23: 'Jaén', 24: 'León', 25: 'Lérida', 26: 'La Rioja',
    27: 'Lugo', 28: 'Madrid', 29: 'Málaga', 30: 'Murcia', 31: 'Navarra', 32: 'Orense',
    33: 'Asturias', 34: 'Palencia', 35: 'Las Palmas', 36: 'Pontevedra', 37: 'Salamanca',
    38: 'Santa Cruz de Tenerife', 39: 'Cantabria', 40: 'Segovia', 41: 'Sevilla',
    42: 'Soria', 43: 'Tarragona', 44: 'Teruel', 45: 'Toledo', 46: 'Valencia',
    47: 'Valladolid', 48: 'Vizcaya', 49: 'Zamora', 50: 'Zaragoza'
}

# Connect to the database
db_path = 'BD_Prediccion.db'
conn = sqlite3.connect(db_path)

# Load the data from the table
query = """
    SELECT
        PROVINCIA_NOMBRE_0, PROVINCIA_NOMBRE_1, PROVINCIA_NOMBRE_2,
        PROVINCIA_NOMBRE_3, PROVINCIA_NOMBRE_4, PROVINCIA_NOMBRE_5,
        NUMERO_ELECTROLINERAS, Prediccion_Cargadores
    FROM Coches_Cargadores_Prediccion
"""
df = pd.read_sql(query, conn)
conn.close()

# Ensure binary columns are integers
binary_columns = ['PROVINCIA_NOMBRE_0', 'PROVINCIA_NOMBRE_1', 'PROVINCIA_NOMBRE_2',
                  'PROVINCIA_NOMBRE_3', 'PROVINCIA_NOMBRE_4', 'PROVINCIA_NOMBRE_5']
for col in binary_columns:
    df[col] = df[col].astype(int)

# Function to decode province number
def decode_province(row):
    # Concatenate the binary encoding columns
    binary_str = "".join(str(int(val)) for val in row[binary_columns])
    # Convert the binary string to an integer
    province_number = int(binary_str, 2)
    # Get the province name from the dictionary
    return provinces_dict.get(province_number, "Unknown")

# Apply the decoding function
df['PROVINCIA_NOMBRE'] = df.apply(decode_province, axis=1)

# Filter out rows for Madrid and Barcelona
df = df[~df['PROVINCIA_NOMBRE'].isin(['Madrid', 'Barcelona'])]

# Display the DataFrame without Madrid and Barcelona
print(df[['PROVINCIA_NOMBRE', 'NUMERO_ELECTROLINERAS', 'Prediccion_Cargadores']])

# Dictionary mapping province numbers to names
provinces_dict = {
    1: 'Álava', 2: 'Albacete', 3: 'Alicante', 4: 'Almería', 5: 'Ávila',
    6: 'Badajoz', 7: 'Islas Baleares', 8: 'Barcelona', 9: 'Burgos', 10: 'Cáceres',
    11: 'Cádiz', 12: 'Castellón', 13: 'Ciudad Real', 14: 'Córdoba', 15: 'La Coruña',
    16: 'Cuenca', 17: 'Gerona', 18: 'Granada', 19: 'Guadalajara', 20: 'Guipúzcoa',
    21: 'Huelva', 22: 'Huesca', 23: 'Jaén', 24: 'León', 25: 'Lérida', 26: 'La Rioja',
    27: 'Lugo', 28: 'Madrid', 29: 'Málaga', 30: 'Murcia', 31: 'Navarra', 32: 'Orense',
    33: 'Asturias', 34: 'Palencia', 35: 'Las Palmas', 36: 'Pontevedra', 37: 'Salamanca',
    38: 'Santa Cruz de Tenerife', 39: 'Cantabria', 40: 'Segovia', 41: 'Sevilla',
    42: 'Soria', 43: 'Tarragona', 44: 'Teruel', 45: 'Toledo', 46: 'Valencia',
    47: 'Valladolid', 48: 'Vizcaya', 49: 'Zamora', 50: 'Zaragoza'
}

# Connect to the database
db_path = 'BD_Prediccion.db'
conn = sqlite3.connect(db_path)

# Load the data from the table
query = """
    SELECT PROVINCIA_NOMBRE_0, PROVINCIA_NOMBRE_1, PROVINCIA_NOMBRE_2,
           PROVINCIA_NOMBRE_3, PROVINCIA_NOMBRE_4, PROVINCIA_NOMBRE_5,
           NUMERO_ELECTROLINERAS, Prediccion_Cargadores
    FROM Coches_Cargadores_Prediccion
"""
df = pd.read_sql(query, conn)
conn.close()

# Ensure binary columns are integers
binary_columns = ['PROVINCIA_NOMBRE_0', 'PROVINCIA_NOMBRE_1', 'PROVINCIA_NOMBRE_2',
                  'PROVINCIA_NOMBRE_3', 'PROVINCIA_NOMBRE_4', 'PROVINCIA_NOMBRE_5']
for col in binary_columns:
    df[col] = df[col].astype(int)

# Function to decode province number
def decode_province(row):
    binary_str = "".join(str(int(val)) for val in row[binary_columns])
    province_number = int(binary_str, 2)
    return provinces_dict.get(province_number, "Unknown")

# Apply the decoding function
df['PROVINCIA_NOMBRE'] = df.apply(decode_province, axis=1)

# Filter out rows for Madrid and Barcelona
df = df[~df['PROVINCIA_NOMBRE'].isin(['Madrid', 'Barcelona'])]

# Sort the DataFrame by province name
df = df.sort_values(by='PROVINCIA_NOMBRE')

# Set the positions and width for the bars
x = range(len(df))
width = 0.35  # Width of the bars

# Create the bar plot
fig, ax = plt.subplots(figsize=(14, 8))
ax.bar([pos - width/2 for pos in x], df['NUMERO_ELECTROLINERAS'], width=width, color='orange', label='Original Chargers')
ax.bar([pos + width/2 for pos in x], df['Prediccion_Cargadores'], width=width, color='green', label='Predicted Chargers')

# Add labels and title
ax.set_xlabel('Province', fontsize=12)
ax.set_ylabel('Number of Charging Stations', fontsize=12)
ax.set_title('Comparison of Original and Predicted Number of Charging Stations by Province', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(df['PROVINCIA_NOMBRE'], rotation=90)
ax.legend()

# Show the plot
plt.tight_layout()
plt.show()

# Dictionary mapping province numbers to names
provinces_dict = {
    1: 'Álava', 2: 'Albacete', 3: 'Alicante', 4: 'Almería', 5: 'Ávila',
    6: 'Badajoz', 7: 'Islas Baleares', 8: 'Barcelona', 9: 'Burgos', 10: 'Cáceres',
    11: 'Cádiz', 12: 'Castellón', 13: 'Ciudad Real', 14: 'Córdoba', 15: 'La Coruña',
    16: 'Cuenca', 17: 'Gerona', 18: 'Granada', 19: 'Guadalajara', 20: 'Guipúzcoa',
    21: 'Huelva', 22: 'Huesca', 23: 'Jaén', 24: 'León', 25: 'Lérida', 26: 'La Rioja',
    27: 'Lugo', 28: 'Madrid', 29: 'Málaga', 30: 'Murcia', 31: 'Navarra', 32: 'Orense',
    33: 'Asturias', 34: 'Palencia', 35: 'Las Palmas', 36: 'Pontevedra', 37: 'Salamanca',
    38: 'Santa Cruz de Tenerife', 39: 'Cantabria', 40: 'Segovia', 41: 'Sevilla',
    42: 'Soria', 43: 'Tarragona', 44: 'Teruel', 45: 'Toledo', 46: 'Valencia',
    47: 'Valladolid', 48: 'Vizcaya', 49: 'Zamora', 50: 'Zaragoza'
}

# Connect to the database
db_path = 'BD_Prediccion.db'
conn = sqlite3.connect(db_path)

# Load the data from the table
query = """
    SELECT PROVINCIA_NOMBRE_0, PROVINCIA_NOMBRE_1, PROVINCIA_NOMBRE_2,
           PROVINCIA_NOMBRE_3, PROVINCIA_NOMBRE_4, PROVINCIA_NOMBRE_5,
           NUMERO_ELECTROLINERAS, Prediccion_Cargadores
    FROM Coches_Cargadores_Prediccion
"""
df = pd.read_sql(query, conn)
conn.close()

# Ensure binary columns are integers
binary_columns = ['PROVINCIA_NOMBRE_0', 'PROVINCIA_NOMBRE_1', 'PROVINCIA_NOMBRE_2',
                  'PROVINCIA_NOMBRE_3', 'PROVINCIA_NOMBRE_4', 'PROVINCIA_NOMBRE_5']
for col in binary_columns:
    df[col] = df[col].astype(int)

# Function to decode province number
def decode_province(row):
    binary_str = "".join(str(int(val)) for val in row[binary_columns])
    province_number = int(binary_str, 2)
    return provinces_dict.get(province_number, "Unknown")

# Apply the decoding function
df['PROVINCIA_NOMBRE'] = df.apply(decode_province, axis=1)

# Filter out rows for Madrid and Barcelona
df = df[~df['PROVINCIA_NOMBRE'].isin(['Madrid', 'Barcelona'])]

# Calculate the difference between predicted and original chargers
df['Difference'] = df['Prediccion_Cargadores'] - df['NUMERO_ELECTROLINERAS']

# Sort the DataFrame by the difference
df_deficit = df.sort_values(by='Difference').head(10)
df_excess = df.sort_values(by='Difference', ascending=False).head(10)

# Display the results using tables
print("Top 10 Provinces with Greatest Excess of Chargers (Deficit):")
print(df_deficit[['PROVINCIA_NOMBRE', 'NUMERO_ELECTROLINERAS', 'Prediccion_Cargadores', 'Difference']])

print("\nTop 10 Provinces with Most Need of Chargers:")
print(df_excess[['PROVINCIA_NOMBRE', 'NUMERO_ELECTROLINERAS', 'Prediccion_Cargadores', 'Difference']])